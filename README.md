# üèÜ poet-ident-RUBert-LSTM

### –°lassification of poetic texts using 2 approaches: pre-trained RuBERT (fine-tuning) and LSTM architecture. PyTorch implementation includes comparison of transformer vs RNN approaches for classification task.
<div align="center">

[![Stars](https://img.shields.io/github/stars/VeronikaKolimova/poet-ident-RUBert-LSTM?style=social)](https://github.com/VeronikaKolimova/poet-ident-RUBert-LSTM/stargazers/)

</div>

<div align="center">

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![HuggingFace](https://img.shields.io/badge/ü§ó-Transformers-ffcc00.svg)](https://huggingface.co)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3-orange)](https://scikit-learn.org/)
[![RuBERT](https://img.shields.io/badge/RuBERT-base--cased-red)](https://huggingface.co/DeepPavlov/rubert-base-cased)
[![License: MIT](https://img.shields.io/badge/License-MIT-green)](https://opensource.org/licenses/MIT)
[![Status Completed](https://img.shields.io/badge/status-completed-brightgreen.svg)]()

</div>

# üåπ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π / Russian Poetry Authorship Classification


## –û—Ç–∫—Ä—ã—Ç—å –Ω–æ—É—Ç–±—É–∫ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/VeronikaKolimova/poet-ident-RUBert-LSTM/blob/main/L_07.ipynb)

## üåç –û–ø–∏—Å–∞–Ω–∏–µ / Description
**RU**: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π (–ë—É–Ω–∏–Ω, –ï—Å–µ–Ω–∏–Ω, –ú–∞—è–∫–æ–≤—Å–∫–∏–π, –¶–≤–µ—Ç–∞–µ–≤–∞) —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ LSTM/BiLSTM –∏ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å RuBERT (DeepPavlov). –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫, –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫. –¢–æ—á–Ω–æ—Å—Ç—å RuBERT: **96.83%**.

**EN**: Authorship classification of Russian poems using neural networks: custom LSTM/BiLSTM and fine-tuned RuBERT (DeepPavlov). Model comparison, metrics visualization, error analysis. RuBERT accuracy: **96.83%**.

## üìä Key Results
| Model | Epochs | Val Accuracy | Val Loss |
|-------|--------|--------------|----------|
| Frozen BERT | 10 | **63.65%** | 1.0997 |
| Fine-tuned | **5** | **96.88%** | **0.1449** |

**Dataset**: 420 Russian poems, 4 authors (294 train / 63 val / 63 test)

## üöÄ Quick Start
```bash
pip install -r requirements.txt
jupyter notebook L_07.ipynb
```

## üíæ Dataset & Model
dataset.csv
RuBERT checkpoint: DeepPavlov/rubert-base-cased[file:62]


## üìà Results Visualization
![Training Curves](https://raw.githubusercontent.com/VeronikaKolimova/poet-ident-RUBert-LSTM/main/images/bert_training_curves.png)
![Confusion Matrix](https://raw.githubusercontent.com/VeronikaKolimova/poet-ident-RUBert-LSTM/main/images/_lstm_cm.png)





